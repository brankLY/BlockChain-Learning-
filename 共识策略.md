# 公链的共识策略，本质上是为了确定记账权
## POW：挖矿记账，算力决定节点记账的可能性。
## POS：股权记账，持有量决定节点记账可能性。
## DPOS：委托权益记账，把持有量代表的记账权利委托给记账节点进行记账。

# 联盟链可能采用的共识算法
## Raft
从[xybaby的博文](https://www.cnblogs.com/xybaby/p/10124083.html)借鉴不少，部分内容照搬过来的。

不可能不推荐的raft演示动画：http://thesecretlivesofdata.com/raft
### 角色
Leader(领导者节点)、Followers（跟随者节点）、Candidate（候选人节点）

正常运行中的Raft网络应该是只存在一个Leader(领导者节点)，和很多个Followers（跟随者节点）。

### 数据结构

#### 所有节点都使用的

| 状态        | 描述 |
| --------   | :----:  |
| currentTerm      |    服务器最后一次知道的任期号（初始化为 0，持续递增）    |
| votedFor|  在当前获得选票的候选人的 Id   |
| log[]	|  日志条目集；每一个条目包含一个用户状态机执行的指令，和收到时的任期号   |
| commitIndex|  已知的最大的已经被提交的日志条目的索引值   |
| lastApplied|  最后被应用到状态机的日志条目索引值（初始化为 0，持续递增）   |

#### 领导者节点使用的
在领导人里经常改变的 （选举后重新初始化）

| 状态        | 描述 |
| --------   | :----:  |
| nextIndex[]      |    对于每一个服务器，需要发送给他的下一个日志条目的索引值（初始化为领导人最后索引值加一）    |
| matchIndex[]|  对于每一个服务器，已经复制给他的日志的最高索引值   |

接收者实现：

如果 term < currentTerm 就返回 false 

如果日志在 prevLogIndex 位置处的日志条目的任期号和 prevLogTerm 不匹配，则返回 false 

如果已经存在的日志条目和新的产生冲突（索引值相同但是任期号不同），删除这一条和之后所有的 

附加任何在已有的日志中不存在的条目

如果 leaderCommit > commitIndex，令 commitIndex 等于 leaderCommit 和 新日志条目索引值中较小的一个

#### 请求投票 RPC

| 参数        | 描述 |
| --------   | :----:  |
| term      |    候选人的任期号    |
| candidateId|  请求选票的候选人的 Id   |
| lastLogIndex|  候选人的最后日志条目的索引值   |
| lastLogTerm|  候选人最后日志条目的任期号   |

| 返回值        | 描述 |
| --------   | :----:  |
| term      |    当前任期号，以便于候选人去更新自己的任期号    |
| voteGranted|  候选人赢得了此张选票时为真   |

接收者实现：

如果term < currentTerm返回 false 

如果 votedFor 为空或者就是 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他

#### 附加日志 RPC

| 参数        | 描述 |
| --------   | :----:  |
| term      |    领导人的任期号    |
| leaderId|  领导人的 Id，以便于跟随者重定向请求   |
| prevLogIndex|  新的日志条目紧随之前的索引值   |
| prevLogTerm|  prevLogIndex 条目的任期号   |
| entries[]|  准备存储的日志条目（表示心跳时为空；一次性发送多个是为了提高效率）   |
| leaderCommit|  领导人已经提交的日志的索引值   |

| 返回值        | 描述 |
| --------   | :----:  |
| term      |    当前任期号，以便于候选人去更新自己的任期号    |
| success|  跟随者包含了匹配上 prevLogIndex 和 prevLogTerm 的日志时为真   |

接收者实现：

如果 term < currentTerm 就返回 false 

如果日志在 prevLogIndex 位置处的日志条目的任期号和 prevLogTerm 不匹配，则返回 false 

如果已经存在的日志条目和新的产生冲突（索引值相同但是任期号不同），删除这一条和之后所有的 

附加任何在已有的日志中不存在的条目

如果 leaderCommit > commitIndex，令 commitIndex 等于 leaderCommit 和 新日志条目索引值中较小的一个

### 总体架构

RAFT将共识（一致性）问题分成了三个问题，其定义领导者节点作为管理者进行主导共识（一致性），所以共识需要解决：

1.领导者节点选举

2.领导者节点主导同步日志

3.在不同情况下保证安全

其核心在于保证n/2+1节点正常就能够提供服务。

### 一些辅助实现的结构

#### Term（任期）
其作用对应绝大部分政治体系里的任期，主要是用于选举leader节点。

每个节点自身具有term编号，初始为0。当有节点的逻辑时钟到期后转换为Candidate节点，Term加1这时Term为1（任期），然后开始选举，这时候有几种情况会使Term发生改变：

    1：如果当前Term的任期内没有选举出Leader或出现异常，则Term递增，开始新一任期选举
　　 

    2：当这轮Term为2的周期选举出Leader后，过后Leader宕掉    了，然后其他Follower转为Candidate，Term递增，开始新一任期选举
　　

    3：当Leader或Candidate发现自己的Term比别的Follower小时Leader或Candidate将转为Follower，Term递增


    4：当Follower的Term比别的Term小时Follower也将更新Term保持与其他Follower一致；

### 领导者节点选举

定时器时间（选举超时时间）：选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。

发生情况：

    1：Raft初次启动，不存在Leader，节点在定时器时间（选举超时时间）后发起选举；
    
    2：Leader宕机或Follower没有接收到Leader的heartBeat，发生election timeout从而发起选举;


选举流程：

    候选人节点视角：

    1:发起选举节点Term+1，并状态由Follower转为Candidate，同时向其他节点发起RequestVote RPC请求投票给自己，而且也会自己给自己投一票

    2:可能发生的情况：

    2.1 该RequestVote请求接收到n/2+1（过半数）个节点的投票，从Candidate转为Leader，向其他节点发送heartBeat以保持Leader的正常运转。
    2.2 在此期间如果收到其他节点发送过来的AppendEntries RPC请求，如该节点的Term大则当前节点转为Follower，否则保持Candidate拒绝该请求。
    2.3 Election timeout发生则Term递增，重新发起选举。
    
    在一个Term期间每个节点只能投票一次，所以当有多个Candidate存在时就会出现每个Candidate发起的选举都存在接收到的投票数都不过半的问题，这时每个Candidate都将Term递增、重启定时器并重新发起选举。为此raft引入了randomized election timeouts来尽量避免平票情况。同时设计的节点的数目通常都是奇数个，尽量保证多数票的出现。

    投票的Follower节点视角：

    当满足以下情况时，Follower节点投票
    1.当前term内还未投票。
    2.检查发现候选人知道的信息不能比自己的少。


### 领导者节点保证同步日志

前提
所有服务器：

如果commitIndex > lastApplied，那么就 lastApplied 加一，并把log[lastApplied]应用到状态机中

如果接收到的 RPC 请求或响应中，任期号T > currentTerm，那么就令 currentTerm 等于 T，并切换状态为跟随者

#### 相关结构

日志（log）：由有序编号（log index）的（log entry）日志条目组成。每个日志条目包含它被创建时的任期号（term），和用于状态机执行的命令（command）。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。

#### 同步流程
    因为Leader向所有Followers周期性发送heartbeat，所以同步时的信息也通过发送heartbeat时伴随发送。

    1.领导者节点从客户端接受到command后创建日志条目
    2.领导者节点并行发送同步日志请求（AppendEntries RPC ）给所以Follower节点
    3.领导者节点等待多数节点同步成功的反馈
    4.领导者节点将这条日志条目应用到它的状态机（即更改本地数据）
    5.领导者节点向客户端返回执行结果。
    6.领导者节点向所有的Follower节点通知应用该日志（即所有follower节点更改本地数据）

    Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。

    Leader为了使Followers的日志同自己的一致，Leader需要找到Followers同它的日志一致的地方，然后覆盖Followers在该位置之后的条目。

    Leader会从后往前试，每次AppendEntries失败后尝试前一个日志条目，直到成功找到每个Follower的日志一致位点，然后向后逐条覆盖Followers在该位置之后的条目。    

### 在不同情况下保证安全

#### 相关概念
    leader election约束：

    同一任期内最多只能投一票，先来先得

    选举人必须比自己知道的更多（比较term，log index）
    
    log replication约束：

    一个log被复制到大多数节点，就是committed，保证不会回滚

    leader一定包含最新的committed log，因此leader只会追加日志，不会删除覆盖日志

    不同节点，某个位置上日志相同，那么这个位置之前的所有日志一定是相同的

#### 情况及解决方法

    一般情况下，Leader和Followers的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，Leader崩溃可能会导致日志不一致：旧的Leader可能没有完全复制完日志中的所有条目。

    Raft的两条特性：
    1.如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。
    2.如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的。
    
    第一条特性源于Leader在一个term内在给定的一个log index最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。

    第二条特性源于 AppendEntries一致性检查。

    Raft增加了如下两条限制以保证安全性：
    1.拥有最新的已提交的log entry的Follower才有资格成为Leader。

    2. Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）。

    1在RequestVote RPC中保证，Candidate在发送RequestVote RPC时，要带上自己的最后一条日志的term和log index，其他节点收到消息时，如果发现自己的日志比请求中携带的更新，则拒绝投票。日志比较的原则是，如果本地的最后一条log entry的term更大，则term大的更新，如果term一样大，则log index更大的更新。

    2用于避免已提交的日志被覆盖的情况。

#### 其他相关问题

##### 日志压缩

由于系统重启时需要花很长的时间进行加载回放日志，Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃。

每个节点独立的进行snapshot，并且只能对已经提交的日志记录进行snapshot。

Snapshot中包含以下内容：

日志元数据。最后一条已提交的 log entry的 log index和term。这两个值在snapshot之后的第一条log entry的AppendEntries RPC的完整性检查的时候会被用上。

系统当前状态。

当Leader要发给某个日志落后太多的Follower的log entry被丢弃，Leader会将snapshot发给Follower。或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC（RPC细节参见八、Raft算法总结）。

##### 成员变更

Raft提出了两阶段的成员变更方法。集群先从旧成员配置Cold切换到一个过渡成员配置，称为共同一致（joint consensus），共同一致是旧成员配置Cold和新成员配置Cnew的组合Cold U Cnew，一旦共同一致Cold U Cnew被提交，系统再切换到新成员配置Cnew。


Raft两阶段成员变更过程如下：

Leader收到成员变更请求从Cold切成Cnew；

Leader在本地生成一个新的log entry，其内容是Cold∪Cnew，代表当前时刻新旧成员配置共存，写入本地日志，同时将该log entry复制至Cold∪Cnew中的所有副本。在此之后新的日志同步需要保证得到Cold和Cnew两个多数派的确认；

Follower收到Cold∪Cnew的log entry后更新本地日志，并且此时就以该配置作为自己的成员配置；

如果Cold和Cnew中的两个多数派确认了Cold U Cnew这条日志，Leader就提交这条log entry；

接下来Leader生成一条新的log entry，其内容是新成员配置Cnew，同样将该log entry写入本地日志，同时复制到Follower上；

Follower收到新成员配置Cnew后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置，并且如果发现自己不在Cnew这个成员配置中会自动退出；

Leader收到Cnew的多数派确认后，表示成员变更成功，后续的日志只要得到Cnew多数派确认即可。Leader给客户端回复成员变更执行成功。



新节点问题：新的服务器没有任何数据，加入进来进来怎么保证系统的可用性（这个时候新日志没办法Commit就没办法响应给客户端）？

新加入的节点需要时间复制数据，在这个过程完成之前，Raft采用以下机制来保证可用性： 新加入节点没有投票权（Leader复制日志给他们，但是不将他们考虑在机器数量里面——即在判断是否超过半数时不把这些节点考虑在内），直到这些节点的日志追上其他节点。


## Paxos算法
### 角色
Proposer（提议者），Acceptor（决策者），Learner（决策学习者）

### 总体架构

将分布式一致拆解为两个阶段：Prepare、Accept


### 同步流程

第一阶段Prepare：
A、Proposer生成一个全局唯一的提案编号N，然后向所有Acceptor发送编号为N的Prepare请求。
B、如果一个Acceptor收到一个编号为N的Prepare请求，且N大于本Acceptor已经响应过的所有Prepare请求的编号，那么本Acceptor就会将其已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时本Acceptor承诺不再接受任何编号小于N的提案。

第二阶段 Accept：
A、如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么Proposer就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V由Proposer自己决定。
B、如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，Acceptor就接受该提案。
Paxos 并不保证系统总处在一致的状态。但由于每次达成共识至少有超过一半的节点参与，最终整个系统都会获知共识结果。如果提案者在提案过程中出现故障，可以通过超时机制来缓解。
Paxos 能保证在超过一半的节点正常工作时，系统总能以较大概率达成共识。


### 存在的问题：
Paxos算法的活锁
如果两个提案者恰好依次提出更新的提案，则导致活锁，系统会永远无法达成共识(实际发生概率很小)。活锁没有产生阻塞，但是一直无法达成一致。
活锁有三种解决方案：
A、在被打回第一阶段再次发起PrepareRequest请求前加入随机等待时间。
B、设置一个超时时间，到达超时时间后，不再接收PrepareRequest请求。
C、在Proposer中选举出一个leader，通过leader统一发出PrepareRequest和AcceptRequest。

### zookeeper实现
要分为三者角色，而每一个节点同时只能扮演一种角色，这三种角色分别是：
(1). Leader 接受所有Follower的提案请求并统一协调发起提案的投票，负责与所有的Follower进行内部的数据交换(同步);
(2). Follower 直接为客户端服务并参与提案的投票，同时与Leader进行数据交换(同步);
(3). Observer 直接为客户端服务但并不参与提案的投票，同时也与Leader进行数据交换(同步);observer的作用是为了拓展系统，提高读取速度。

LOOKING：竞选状态，当前Server不知道leader是谁，正在搜寻。
LEADING：领导者状态，表明当前服务器角色是leader。
FOLLOWING：随从状态，表明当前服务器角色是follower，同步leader状态，参与投票。
OBSERVING，观察状态，表明当前服务器角色是observer，同步leader状态，不参与投票。

#### 基本结构
sid:即server id，用来标识该机器在集群中的机器序号。
zxid:即zookeeper事务id号。ZooKeeper状态的每一次改变, 都对应着一个递增的Transaction id, 该id称为zxid. 由于zxid的递增性质, 如果zxid1小于zxid2, 那么zxid1肯定先于zxid2发生. 创建任意节点, 或者更新任意节点的数据, 或者删除任意节点, 都会导致Zookeeper状态发生改变, 从而导致zxid的值增加.
以（sid，zxid）的形式来标识一次投票信息。例如，如果当前服务器要推举sid为1，zxid为8的服务器成为leader，那么投票信息可以表示为（1，8）

选举规则
集群中的每台机器发出自己的投票后，也会接受来自集群中其他机器的投票。每台机器都会根据一定的规则，来处理收到的其他机器的投票，以此来决定是否需要变更自己的投票。
规则如下：
（1）初始阶段，都会给自己投票。
（2）当接收到来自其他服务器的投票时，都需要将别人的投票和自己的投票进行pk，规则如下：
优先检查zxid。zxid比较大的服务器优先作为leader。
如果zxid相同的话，就比较sid，sid比较大的服务器作为leader。

当一个Server启动时它都会发起一次选举，此时由选举线程发起相关流程，那么每个 Server都会获得当前zxid最大的哪个Server是谁，如果当次最大的Server没有获得n/2+1 个票数，那么下一次投票时，他将向zxid最大的Server投票，重复以上流程，最后一定能选举出一个Leader。

选举线程由当前Server发起选举的线程担任，他主要的功能对投票结果进行统计，并选出推荐的Server。选举线程首先向所有Server发起一次询问(包括自己)，被询问方，根据自己当前的状态作相应的回复，选举线程收到回复后，验证是否是自己发起的询问(验证xid 是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议 的

leader 相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中，当向所有Server
都询问完以后，对统计结果进行筛选并进行统计，计算出当次询问后获胜的是哪一个Server，并将当前zxid最大的Server 设置为当前Server要推荐的Server(有可能是自己，也有可以是其它的Server，根据投票结果而定，但是每一个Server在第一次投票时都会投自己)，如果此时获胜的Server获得n/2 + 1的Server票数，设置当前推荐的leader为获胜的Server。根据获胜的Server相关信息设置自己的状态。每一个Server都重复以上流程直到选举出Leader。


初始化选票(第一张选票): 每个quorum节点一开始都投给自己;

收集选票: 使用UDP协议尽量收集所有quorum节点当前的选票(单线程/同步方式)，超时设置200ms;

统计选票: 1).每个quorum节点的票数;

         2).为自己产生一张新选票(zxid、myid均最大);

选举成功: 某一个quorum节点的票数超过半数;

更新选票: 在本轮选举失败的情况下，当前quorum节点会从收集的选票中选取合适的选票(zxid、myid均最大)作为自己下一轮选举的投票;

